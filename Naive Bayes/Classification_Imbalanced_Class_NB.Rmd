---
title: "Classification: Imbalanced Class"

output: html_notebook
---
###Rebalance: Seismic Data
###Classification: Naive Bayes


The task of the classification model, is to predict the label or class of the target variable, for a given set of predictor variables which are unlabeled (un-classified). Normally, when examining Classification and its efficacy, the data used tends to be well-behaved (no missing data, balanced classes, etc..), consider the dataset that has Seismic Data recorded at a Polish coal mine.

This is a very imbalanced dataset:

<blockquote style="font-size: 15px; font-style: italic;">
The presented dataset is characterized by unbalanced distribution of positive and negative examples.
In the data set there are only 170 positive examples representing class 1.[^2]
</blockquote>

```{r}
library(foreign)
```



###Read the Data
```{r}
seismicData <- read.arff("data/seismic-bumps.arff")

summary(seismicData)

summary(seismicData$class)

#check classes distribution
prop.table(table(seismicData$class))

table(seismicData$class)/nrow(seismicData)*100
```

<br />
More than 93% of the target variable (class) is negative (0) or "non-hazardous", which it should be, since the "shift" predictor variable (column 3) shows more mining activity (W - coal getting) than the provisiong activity (N - preperation). Provisioning activity, would include blasting, moving heavy machinery, etc..

<blockquote style="font-size: 15px; font-style: italic;">
Unbalanced distribution of positive ('hazardous state') and negative
('non-hazardous state') examples is a serious problem in seismic hazard prediction.
<br /><br />
In the data set each row contains a summary statement about seismic activity in the rock mass within
one shift (8 hours). If decision attribute has the value 1, then in the next shift any seismic bump
with an energy higher than 10^4 J was registered.
</blockquote>

Thus, the main intent of this exercise is to try re-balance the data [^3]. Evaluate the classification (the target class is a categorical variable) and document the improvement in predicting if an increase in seismic activity can result in a rockburst.


###Rebalancing
I am going to use the <a href="http://mlr-org.github.io/mlr-tutorial/release/html/index.html">mlr</a> and <a href="https://cran.r-project.org/web/packages/ROSE/index.html">ROSE</a> CRAN libraries. I would have preffered to use just one library, <b>mlr</b>, but unfortunately the logic of combined over-under sampling is yet to be implements (as of current release v2.9). ROSE has this composite over-under sampling method, which I am going to apply here.

The steps I am going to follow are:
<ul>
  <li>Oversample, Undersample and Balancesample the whole dataset</li>
  <li>Split these 3 datasets into training and testing sub-sets</li>
  <li>Apply RPART, Naive Bayes and SVM classifiers to the respective datasets and determine which is better</li>
</ul>




```{r}
library(mlr)
library(ROSE)
```
Split the Seismic Data into training and testing set

```{r}
sample_index <- sample(2, 
                       nrow(seismicData),
                       replace = T,
                       prob = c(0.75,0.25))

#
#
seismicData_train <- seismicData[sample_index ==1 ,]

#
#
seismicData_test <- seismicData[sample_index == 2,]

#
#
table(seismicData_train$class);table(seismicData_test$class)
```


The first step when using the mlr package, is to define a task and perform the sampling on the training set. To overcome the lack of balanced sampling, I used the ROSE package
```{r}
task <- makeClassifTask(data = seismicData_train, target = "class")

task_over <- oversample(task, rate = 14)

task_under <- undersample(task, rate = 1/14)

seismicData_balanced <- ovun.sample(class ~ ., data = seismicData_train, method = "both", p = 0.5)$data

task2 <- makeClassifTask(data = seismicData_balanced, target = "class")


```

```{r}
table(getTaskTargets(task)); table(getTaskTargets(task_over))

table(getTaskTargets(task_under)); table(getTaskTargets(task2))
```


<br /><br />

###Naive Bayes
RPART stands for <b>R</b>ecursive <b>PART</b>itioning, a tree-based method of partitioning. Under the hood it implements constructs articulated in the CART (Classification and Regression Trees) book and programs of Breiman, Friedman, Olshen and Stone.[^4],[^5]





<br /><br />
Define a Naive Bayes learner, train it on the sampled training set and perform the classification on the testing set.

```{r}
lrn_NB <- makeLearner("classif.naiveBayes", predict.type = "prob")

model_NB <- train(lrn_NB, task)

model_NB2 <- train(lrn_NB, task2) #balanced sampling

model_NB_over <- train(lrn_NB, task_over) #over sampling

model_NB_under <- train(lrn_NB, task_under) #under sampling
```

```{r}
performance(predict(model_NB, newdata = seismicData_test), measures = list(mmce, ber, auc))
```



```{r}
performance(predict(model_NB2, newdata = seismicData_test), measures = list(mmce, ber, auc))
```

```{r}
performance(predict(model_NB_over, newdata = seismicData_test), measures = list(mmce, ber, auc))
```

```{r}
performance(predict(model_NB_under, newdata = seismicData_test), measures = list(mmce, ber, auc))
```


\begin{equation}
E = m c^2
\end{equation} 

\begin{equation}
E = m c^3
\end{equation}

$$
E = m c^2
$$

$$
P(A\enspace|\enspace B) = \frac{P(B\enspace|\enspace A)P(A)}{P(B)}
$$

$$
P(A\,|\, B) = \frac{P(B\,|\, A)P(A)}{P(B)}
$$

                    
Naive Bayes is a probabilistic classifier based on the Bayes theorem, shown below.

$$P(A\, |\, B ) = {P(B\, |\, A )\,P(A) \over P(B)}$$

                    
Where:
<ul>
<li>
$A$ is the target or class, $B$ is the predictor or attribute
</li>
<li>
$P(A\,|\,B)$ is defined as the probability of observing $A$ given that $B$ occurs, it is termed as <b>posterior probability</b>
</li>
<li>
$P(B\,|\,A)$ is defined as the probability of observing $B$ given that $A$ occurs, it is termed as <b>likelihood</b>
</li>
<li>
$P(A)$ is defined as the prior probability of the target or class
</li>
<li>
$P(B)$ is defined as the prior probability of the predictor or attribute
</li>
</ul>


<br />
<p>
But, where it deviates from Bayes is the strong conjecture, on the independence between the predictors. In real-world data or situations,these assumptions are seldom valid, hence the prefix Naive (having or showing a lack of experience, judgment, or information).
</p>
                    
<p>
With this context, the Naive Bayes classifier calculates the probability of each target or class for a given group of predictors or observations:
</p>
$$P(c\, |\, {X_1, X_2,\ldots,X_p} ) = {P({X_1, X_2,\ldots,X_p}\, |\, c )\,P(c) \over P({X_1, X_2,\ldots,X_p})}$$

                    
Where:
<ul>
<li>
$c$ is the target or class
</li>
<li>
${X_1, X_2,\ldots,X_p}$ is the predictors or attributes of new instance of data
</li>
<li>
$P(c\, |\, {X_1, X_2,\ldots,X_p} )$ is defined as the probability of observing  target $c$ given that ${X_1, X_2,\ldots,X_p}$ occurs, it is termed as <b>posterior probability</b>
</li>
<li>
$P({X_1, X_2,\ldots,X_p}\, |\, c )$ is the <b>likelihood</b>of the test data occurring given the target $c$
</li>
<li>
$P(c)$ is defined as the prior probability of the target
</li>
<li>
$P({X_1, X_2,\ldots,X_p})$ is defined as the probability of the predictors
</li>
</ul>
<br />
<p>Primarily, the equation above is applied to all possible target values to determine the most probable target of the test instance. The numerator of the equation is the decision maker, as the denominator will be constant over all target. This means that the most probable target for a given test instance is the target $c$ that maximizes the expression $P({X_1, X_2,\ldots,X_p}\, |\, c )$$P(c)$
</p>
<p>
Going back to the naive assumption, that the values taken by a predictor is not dependent on the values of other predictors in the instance for any given target, the probability of observing a target value for the predictors can be obtained by multiplying the probabilities of individual predictors given the target value.
</p>
<div align="center">
$P({X_1, X_2,\ldots,X_p}\, |\, c )$ = $P({X_1}\, |\, c )$$P({X_2}\, |\, c )$$\ldots$$P({X_p}\, |\, c )$ = ${\displaystyle \prod_{i=1}^{p} P({X_i}\, |\, c )}$
</div>

Which then leads to
<div align="center">
$P(c)\,$$P({X_1, X_2,\ldots,X_p}\, |\, c )$ = $P(c)\,$${\displaystyle \prod_{i=1}^{p} P({X_i}\, |\, c )}$
</div>
<p>
The learning process, in the Naive Bayes algorithm, comprises of finding the probabilities $P(c_j)$ and $P({X_i}\, | \, {c_j})$ for all probable values taken by the predictor and target variables based on the training dataset provided.</p>
<p>
$P(c_j)$ is determined as a ratio of the number of times the value $c_j$ occurs in the target attribute to the total number of rows or instances in the training dataset.</p>
<p>Likewise, for a predictor variable at the $i^{th}$ position with a value of $c_j$, probability $P({X_i}\, | \, {c_j})$ is obtained from the number of times ${X_i}$ is seen in the training set when the target value is $c_j$</p>
<p>
<b>Note of caution</b>the Naive Bayes algorithm can only handle discrete and complete (no missing or NA data) predictor variables in the dataset. If data is missing then we need to impute them or drop the rows that have missing data(not advisable since the classifier will have a smaller dataset and hence the performance will be affected). As, for continuous variables, they must be discretized prior to being useful.
</p>



                   

\documentclass{article}
\usepackage{amsmath}      % for \tag and \eqref macros
\setlength\textwidth{7cm} % just for this example
\begin{document}
\[
1+1=2 \tag{5.23} \label{eq:5.23}
\]
A cross-reference to equation \eqref{eq:5.23}.
\end{document}



###<i>Reference</i>
[^1]: Alpaydin, Ethem (2014). Introduction to Machine Learning, Third Edition. MIT Press. ISBN 978-0-262-02818-9.


[^2]: UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/seismic-bumps)

[^3]: Learning from Imbalanced Classes (https://svds.com/learning-imbalanced-classes/)

[^4]: L. Breiman, J.H. Friedman, R.A. Olshen, , and C.J Stone. Classi cation and Regression Tree.  Wadsworth, Belmont, Ca, 1983.

[^5]: T.M Therneau and E.J Atkinson. An Introduction to Recursive Partitioning Using the RPART Routines, Mayo Clinic, 2016.(https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf)


