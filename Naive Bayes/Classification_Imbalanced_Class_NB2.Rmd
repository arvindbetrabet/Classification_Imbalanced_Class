---
title: "Classification: Imbalanced Class"

output: html_notebook
---
###Rebalance: Seismic Data
###Classification: Naive Bayes
<br /><br />
In my previous <a href="https://github.com/arvindbetrabet/Classification_Imbalanced_Class_R/blob/master/RPART/Classification_Imbalanced_Class_RPART.Rmd">R-notebook</a> I delt with RPART classifier, post re-balancing of imbalance Seismic Data.

Here I am using Naive Bayes classifier, following the same process (reading, re-sampling and then classifying) so that each R-Notebooks can be used independently.

The Seismic data, recorded at a Polish coal mine is imbalanced, characterized by unbalanced distribution of positive and negative examples. In the data set there are only 170 positive examples representing class 1.[^1]


```{r}
library(foreign)
library(mlr)
library(ROSE)
library(FSelector)
set.seed(4567)
```



###Read the Data
```{r}
seismicData <- read.arff("data/seismic-bumps.arff")

summary(seismicData)

summary(seismicData$class)

# Remove the predictor variables that are constant
# or have zero variance
seismicData <- removeConstantFeatures(seismicData)

# Check the target variable class distribution
prop.table(table(seismicData$class))

# As percentage
table(seismicData$class)/nrow(seismicData)*100
```





<br />
More than 93% of the target variable (class) is negative (0) or "non-hazardous", which it should be, since the "shift" predictor variable (column 3) shows more mining activity (W - coal getting) than the provisioning activity (N - preparation). Provisioning activity, would include blasting, moving heavy machinery, etc..

But,

<blockquote style="font-size: 13px; font-style: italic;">
Unbalanced distribution of positive ('hazardous state') and negative
('non-hazardous state') examples is a serious problem in seismic hazard prediction.
<br /><br />
In the data set each row contains a summary statement about seismic activity in the rock mass within
one shift (8 hours). If decision attribute has the value 1, then in the next shift any seismic bump
with an energy higher than $10^4$ J was registered.
</blockquote>

Thus, the main intent of this exercise is to re-balance the Seismic data. Evaluate the classification (the target class is a categorical variable) and document the improvement in predicting if an increase in seismic activity can result in a rock burst.


###Rebalancing
I am going to use the <a href="http://mlr-org.github.io/mlr-tutorial/release/html/index.html">mlr</a> and <a href="https://cran.r-project.org/web/packages/ROSE/index.html">ROSE</a> CRAN libraries. I would have preffered to use just one library, <b>mlr</b>, but unfortunately the logic of combined over-under sampling is yet to be implements (as of current release v2.9). ROSE has this composite over-under sampling method, which I am going to apply here.

The steps I am going to follow are:
<ul>
  <li>Oversample, Undersample, Balancesample and SMOTEsample the training subset</li>
  <li>Apply Naive Bayes classifier to the respective subsets, for this blog post</li>
  <li>RPART classifier appeared in a <a href="https://github.com/arvindbetrabet/Classification_Imbalanced_Class_R/blob/master/RPART/Classification_Imbalanced_Class_RPART.Rmd">previous</a> post</li>
  <li>Apply SVM classifiers to the respective datasets, to be covered in subsequent blog post</li>
  <li>Determine which of the 3 classifiers provide better results, as a concluding blog post, for this series</li>
</ul>





Split the Seismic Data into training and testing set
```{r}
sample_index <- sample(2, 
                       nrow(seismicData),
                       replace = T,
                       prob = c(0.75,0.25))
#
#
seismicData_train <- seismicData[sample_index ==1 ,]
#
#
seismicData_test <- seismicData[sample_index == 2,]

```

Check the distribution of the target variable class, in the training set
```{r}
# As numbers
table(seismicData_train$class)

# As percentage
table(seismicData_train$class)/nrow(seismicData_train)*100

```

The test set has the following distribution.
```{r}
# As numbers
table(seismicData_test$class)

# As percentage
table(seismicData_test$class)/nrow(seismicData_test)*100
```
Notice the training and test sets, have nearly the same distribution of the predictor variable, class.

<br />

The first step when using the mlr package, is to define a task and perform the sampling on the training set. To overcome the lack of balanced sampling, I used the ROSE package.

Reference[^2] has a visual explanation to aid the concept of over and under sampling.

In principle, over-sampling is where the minority class (positive/1 in this dataset) is randomly sampled and its number is augmented to bring it on-par with the majority class (negative/0 in this dataset). Under-sampling on the other hand, randomly selects data from the majority class and discards them, so that both the classes even out, as seen in the next section. Balanced sampling is a combination of both, under and over sampling.

After some trial-and-error, the rate setting for the over and under sampling was chosen so as to have the negative and positive class numbers as close as possible (as seen by the distribution)
```{r}
#Define the basic classification task
task_train <- makeClassifTask(data = seismicData_train, target = "class")

#Over sampling task
task_train_over <- oversample(task_train, rate = 13.4)

#Under sampling task
task_train_under <- undersample(task_train, rate = 1/13.4)

#Balanced sampling using the ROSE package, combination of over and under sampling
seismicData_train_balanced <- ovun.sample(class ~ ., data = seismicData_train, method = "both", p = 0.5)$data

#Balanced sampling task
task_train_balanced <- makeClassifTask(data = seismicData_train_balanced, target = "class")

#SMOTE(Synthetic Minority Oversampling Technique)
task_train_smote <- smote(task_train, rate = 13.4, nn = 10)
```

Distribution of training set, unbalanced.
```{r}
# As numbers
table(getTaskTargets(task_train))

# As percentage
table(getTaskTargets(task_train))/nrow(seismicData_train)*100

```
Distribution of training set, over-sampled.
```{r}
# As numbers
table(getTaskTargets(task_train_over))

# As percentage
table(getTaskTargets(task_train_over))/nrow(seismicData_train)*100
```


Distribution of training set, under-sampled.
```{r}
# As numbers
table(getTaskTargets(task_train_under))

# As percentage
table(getTaskTargets(task_train_under))/nrow(seismicData_train)*100
```


Distribution of training set, balance-sampled.
```{r}
table(getTaskTargets(task_train_balanced))

table(getTaskTargets(task_train_balanced))/nrow(seismicData_train)*100
```

Distribution of training set, SMOTE-sampled.
```{r}
table(getTaskTargets(task_train_smote))

table(getTaskTargets(task_train_smote))/nrow(seismicData_train)*100
```


Notice the training set, post-sampling, has nearly the same distribution for the majority and minority predictor variable, class. This suggests that the sampling was successful. Lets move on to using Naive Bayes to perform the classification and see what is the performance when the data is balanced by the 4 sampling processes.




<br /><br />

###Naive Bayes
This section covers a brief overview of the Naive Bayes classifier, which is well documented in textbooks, journals and papers, and is included here to present a cogent view.

Naive Bayes is a probabilistic classifier based on the Bayes theorem, shown below.

$$P(A\, |\, B ) = {P(B\, |\, A )\,P(A) \over P(B)}$$

                    
Where:
<ul>
<li>
$A$ is the target or class, $B$ is the predictor or attribute
</li>
<li>
$P(A\,|\,B)$ is defined as the probability of observing $A$ given that $B$ occurs, it is termed as <b>posterior probability</b>
</li>
<li>
$P(B\,|\,A)$ is defined as the probability of observing $B$ given that $A$ occurs, it is termed as <b>likelihood</b>
</li>
<li>
$P(A)$ is defined as the prior probability of the target or class
</li>
<li>
$P(B)$ is defined as the prior probability of the predictor or attribute
</li>
</ul>


<br />
<p>
But, where it deviates from Bayes is the strong conjecture, on the independence between the predictors. In real-world data or situations,these assumptions are seldom valid, hence the prefix Naive (having or showing a lack of experience, judgment, or information).
</p>
                    
<p>
With this context, the Naive Bayes classifier calculates the probability of each target or class for a given group of predictors or observations:
</p>
$$P(c\, |\, {X_1, X_2,\ldots,X_p} ) = {P({X_1, X_2,\ldots,X_p}\, |\, c )\,P(c) \over P({X_1, X_2,\ldots,X_p})}$$

                    
Where:
<ul>
<li>
$c$ is the target or class
</li>
<li>
${X_1, X_2,\ldots,X_p}$ is the predictors or attributes of new instance of data
</li>
<li>
$P(c\, |\, {X_1, X_2,\ldots,X_p} )$ is defined as the probability of observing  target $c$ given that ${X_1, X_2,\ldots,X_p}$ occurs, it is termed as <b>posterior probability</b>
</li>
<li>
$P({X_1, X_2,\ldots,X_p}\, |\, c )$ is the <b>likelihood</b>of the test data occurring given the target $c$
</li>
<li>
$P(c)$ is defined as the prior probability of the target
</li>
<li>
$P({X_1, X_2,\ldots,X_p})$ is defined as the probability of the predictors
</li>
</ul>
<br />
<p>Primarily, the equation above is applied to all possible target values to determine the most probable target of the test instance. The numerator of the equation is the decision maker, as the denominator will be constant over all target. This means that the most probable target for a given test instance is the target $c$ that maximizes the expression $P({X_1, X_2,\ldots,X_p}\, |\, c )$$P(c)$
</p>
<p>
Going back to the naive assumption, that the values taken by a predictor is not dependent on the values of other predictors in the instance for any given target, the probability of observing a target value for the predictors can be obtained by multiplying the probabilities of individual predictors given the target value.
</p>
<div align="center">
$P({X_1, X_2,\ldots,X_p}\, |\, c )$ = $P({X_1}\, |\, c )$$P({X_2}\, |\, c )$$\ldots$$P({X_p}\, |\, c )$ = ${\displaystyle \prod_{i=1}^{p} P({X_i}\, |\, c )}$
</div>

Which then leads to
<div align="center">
$P(c)\,$$P({X_1, X_2,\ldots,X_p}\, |\, c )$ = $P(c)\,$${\displaystyle \prod_{i=1}^{p} P({X_i}\, |\, c )}$
</div>
<p>
The learning process, in the Naive Bayes algorithm, comprises of finding the probabilities $P(c_j)$ and $P({X_i}\, | \, {c_j})$ for all probable values taken by the predictor and target variables based on the training dataset provided.</p>
<p>
$P(c_j)$ is determined as a ratio of the number of times the value $c_j$ occurs in the target attribute to the total number of rows or instances in the training dataset.</p>
<p>Likewise, for a predictor variable at the $i^{th}$ position with a value of $c_j$, probability $P({X_i}\, | \, {c_j})$ is obtained from the number of times ${X_i}$ is seen in the training set when the target value is $c_j$</p>
<p>
<b>Note of caution</b>the Naive Bayes algorithm can only handle discrete and complete (no missing or NA data) predictor variables in the dataset. If data is missing then we need to impute them or drop the rows that have missing data(not advisable since the classifier will have a smaller dataset and hence the performance will be affected). As, for continuous variables, they must be discretized prior to being useful.
</p>


<br />
Next steps are to, define a Naive Bayes learner, train it on the sampled training set and perform the classification on the testing set.

```{r}
learner_nb <- makeLearner("classif.naiveBayes", predict.type = "prob")
```


Train on the imbalanced training data.
```{r}
model_nb_train <- train(learner_nb, task_train)
```


Train on the balance-sampled training data.
```{r}
model_nb_train_balanced <- train(learner_nb, task_train_balanced) #balanced sampling
```


Train on the over-sampled training data.
```{r}
model_nb_train_over <- train(learner_nb, task_train_over) #over sampling
```


Train on the under-sampled training data.
```{r}
model_nb_train_under <- train(learner_nb, task_train_under) #under sampling
```

Train on the SMOTE-sampled training data.
```{r}
model_nb_train_smote <- train(learner_nb, task_train_smote) #SMOTE sampling
```

Now, using the test set, perform the classification prediction and evaluate the performance. There a variety of performance measures [^3], but I have chosen the following:
<ul>
  <li>mmce - Mean mis-classification error</li>
  <li>ber - Balanced error rate</li>
  <li>auc - Area under the curve</li>
</ul>

```{r}

# Performance of the model using the un-balanced training set.
performance(predict(model_nb_train, newdata = seismicData_test), measures = list(mmce, ber, auc))
```


```{r}
# Performance of the model using the balanced-sampling balanced training set.
performance(predict(model_nb_train_balanced, newdata = seismicData_test), measures = list(mmce, ber, auc))
```


```{r}
# Performance of the model using the over-sampling balanced training set.
performance(predict(model_nb_train_over, newdata = seismicData_test), measures = list(mmce, ber, auc))
```


```{r}
# Performance of the model using the under-sampling balanced training set.
performance(predict(model_nb_train_under, newdata = seismicData_test), measures = list(mmce, ber, auc))
```

```{r}
# Performance of the model using the SMOTE-sampling balanced training set.
performance(predict(model_nb_train_smote, newdata = seismicData_test), measures = list(mmce, ber, auc))
```


Even though, the performance of the model using un-balanced dataset, has low mmce (0.63) and modest auc (0.77), it certainly hides the imbalance of the predictor class. 

The performance results indicate that the model using under-sampling of the imbalanced dataset does better than the others, in light of the following from the developers of the mlr package.

<blockquote style="font-size: 13px; font-style: italic;">
Performance measure has to be considered very carefully.
<br />
As the misclassification rate (mmce) evaluates the overall accuracy of the predictions, the balanced error rate (ber) and area under the ROC Curve (auc) might be more suitable here, as the misclassifications within each class are separately taken into account.
</blockquote>









###<i>References</i>

[^1]: UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/seismic-bumps)

[^2]: Learning from Imbalanced Classes (https://svds.com/learning-imbalanced-classes/)

[^3]: Implemented Performance Measures, Machine Learning in R. (http://mlr-org.github.io/mlr-tutorial/release/html/measures/index.html)


